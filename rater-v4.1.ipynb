{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gloria/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/gloria/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input, Dropout, Activation\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_vggface import utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 200\n",
    "IMG_DIR = '../project/all_females'\n",
    "RATING_PATH = './Projekt_SGE_Assessment_ErikK.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable weights:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 200, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 200, 200, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 200, 200, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 100, 100, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 100, 100, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 50, 50, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 50, 50, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 50, 50, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 25, 25, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 25, 25, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 25, 25, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 14,879,041\n",
      "Trainable params: 164,353\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#create function for calling clipped ReLU\n",
    "def create_relu_advanced(max_value=1.):        \n",
    "    def relu_advanced(x):\n",
    "        return K.relu(x, max_value=K.cast_to_floatx(max_value))\n",
    "    return relu_advanced\n",
    "\n",
    "vgg_base = VGGFace(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='max')\n",
    "\n",
    "# Add custom layers\n",
    "last_layer = vgg_base.get_layer('global_max_pooling2d_1').output\n",
    "#X = Dropout(0.2)(last_layer)\n",
    "# x = Flatten(name='flatten')(last_layer)\n",
    "X = Dense(256, activation='relu', name='fc6')(last_layer)\n",
    "X = Dropout(0.2)(X)\n",
    "X = Dense(128, activation='relu', name='fc7')(X)\n",
    "X = Dropout(0.2)(X)\n",
    "output = Dense(1, activation=create_relu_advanced(max_value=5))(X)\n",
    "#output = Dense(1, activation='relu')(X)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=vgg_base.input, outputs=output)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in vgg_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Make sure weights are not trainable\n",
    "print(\"Trainable weights:\")\n",
    "model.trainable_weights\n",
    "\n",
    "model.compile(optimizer=Adam(clipnorm=1.0),loss='mean_absolute_error', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "### **Please remember to delete the train & test folders if you have run this cell before!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test-split:\n",
      "X_train:  (454,)\n",
      "X_test:  (114,)\n",
      "y_train:  (454,)\n",
      "y_test:  (114,)\n"
     ]
    }
   ],
   "source": [
    "# Read in ratings\n",
    "ratings = np.genfromtxt(RATING_PATH)\n",
    "\n",
    "### Distribute images across folders\n",
    "if not os.path.isdir('./data/train/train'):\n",
    "    os.makedirs('./data/train/train')\n",
    "if not os.path.isdir('./data/test/test'):\n",
    "    os.makedirs('./data/test/test')\n",
    "        \n",
    "### Move all images according to ratings\n",
    "images = sorted(os.listdir(IMG_DIR))\n",
    "\n",
    "### Make sure lengths of ratings and images correspond \n",
    "assert len(ratings) == len(images)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, ratings, test_size=0.2)\n",
    "    \n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "        \n",
    "print(\"Train-test-split:\")\n",
    "print(\"X_train: \",X_train.shape)\n",
    "print(\"X_test: \",X_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "data_train = np.zeros((X_train.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "data_test = np.zeros((X_test.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "for idx, _im in enumerate(X_train):\n",
    "    # Change the image path with yours.\n",
    "    _img = image.load_img(os.path.join(IMG_DIR, _im), target_size=(IMG_SIZE,IMG_SIZE))\n",
    "    _x = image.img_to_array(_img)\n",
    "    _x = np.expand_dims(_x, axis=0)\n",
    "    data_train[idx, :, :, :] = utils.preprocess_input(_x, version=1) / .255 \n",
    "    \n",
    "# Same for test data\n",
    "for idx, _im in enumerate(X_test):   \n",
    "    _img = image.load_img(os.path.join(IMG_DIR, _im), target_size=(IMG_SIZE,IMG_SIZE))\n",
    "    _x = image.img_to_array(_img)\n",
    "    _x = np.expand_dims(_x, axis=0)\n",
    "    data_test[idx, :, :, :] = utils.preprocess_input(_x, version=1) / .255\n",
    "\n",
    "### Loop throgh all images and store them in the dedicated folders\n",
    "#for img in X_train:\n",
    "#    src = os.path.join(IMG_DIR, img)\n",
    "#    dest = os.path.join('./data/train/train', img)\n",
    "#    shutil.copy(src, dest)\n",
    "\n",
    "#for img in X_test:\n",
    "#    src = os.path.join(IMG_DIR, img)\n",
    "#    dest = os.path.join('./data/test/test', img)\n",
    "#    shutil.copy(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, \n",
    "                             rotation_range=40,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode='nearest',\n",
    "                             preprocessing_function=utils.preprocess_input)\n",
    "\n",
    "datagen.fit(data_train)\n",
    "\n",
    "#train_generator = datagen.flow_from_directory(\n",
    "#    './data/train',\n",
    "#    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "#    batch_size=64,\n",
    "#    class_mode=None\n",
    "#)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen.fit(data_test)\n",
    "\n",
    "#test_generator = test_datagen.flow_from_directory(\n",
    "#    './data/test',\n",
    "#    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "#    batch_size=64,\n",
    "#    class_mode=None\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 76s 760ms/step - loss: 1.1930 - mean_absolute_error: 1.1930 - val_loss: 1.2009 - val_mean_absolute_error: 1.2009\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.20090, saving model to ./weights/weights-v4.1.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./weights/weights-v4.1.hdf5', verbose=1, save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(data_train, y_train, batch_size=64),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=1,\n",
    "    validation_data=test_datagen.flow(data_test, y_test, batch_size=64),\n",
    "    callbacks=[checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7/7 [==============================] - 6s 834ms/step - loss: 1.1677 - mean_absolute_error: 1.1677 - val_loss: 1.2122 - val_mean_absolute_error: 1.2122\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 5s 779ms/step - loss: 1.1605 - mean_absolute_error: 1.1605 - val_loss: 1.2265 - val_mean_absolute_error: 1.2265\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 5s 778ms/step - loss: 1.0853 - mean_absolute_error: 1.0853 - val_loss: 1.2068 - val_mean_absolute_error: 1.2068\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 6s 850ms/step - loss: 1.0247 - mean_absolute_error: 1.0247 - val_loss: 1.2195 - val_mean_absolute_error: 1.2195\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 5s 778ms/step - loss: 1.1812 - mean_absolute_error: 1.1812 - val_loss: 1.2270 - val_mean_absolute_error: 1.2270\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 5s 735ms/step - loss: 1.1768 - mean_absolute_error: 1.1768 - val_loss: 1.2146 - val_mean_absolute_error: 1.2146\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 6s 858ms/step - loss: 1.0952 - mean_absolute_error: 1.0952 - val_loss: 1.2207 - val_mean_absolute_error: 1.2207\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 5s 780ms/step - loss: 1.0955 - mean_absolute_error: 1.0955 - val_loss: 1.1978 - val_mean_absolute_error: 1.1978\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.20090 to 1.19778, saving model to ./weights/weights-v4.1.hdf5\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 5s 780ms/step - loss: 1.0693 - mean_absolute_error: 1.0693 - val_loss: 1.1663 - val_mean_absolute_error: 1.1663\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.19778 to 1.16635, saving model to ./weights/weights-v4.1.hdf5\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 5s 772ms/step - loss: 1.0239 - mean_absolute_error: 1.0239 - val_loss: 1.2078 - val_mean_absolute_error: 1.2078\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 5s 771ms/step - loss: 1.0178 - mean_absolute_error: 1.0178 - val_loss: 1.2079 - val_mean_absolute_error: 1.2079\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 6s 857ms/step - loss: 0.9643 - mean_absolute_error: 0.9643 - val_loss: 1.2087 - val_mean_absolute_error: 1.2087\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 5s 773ms/step - loss: 0.9974 - mean_absolute_error: 0.9974 - val_loss: 1.2197 - val_mean_absolute_error: 1.2197\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 5s 776ms/step - loss: 0.8777 - mean_absolute_error: 0.8777 - val_loss: 1.1436 - val_mean_absolute_error: 1.1436\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.16635 to 1.14360, saving model to ./weights/weights-v4.1.hdf5\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 5s 772ms/step - loss: 1.0425 - mean_absolute_error: 1.0425 - val_loss: 1.1459 - val_mean_absolute_error: 1.1459\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 5s 776ms/step - loss: 1.0523 - mean_absolute_error: 1.0523 - val_loss: 1.1759 - val_mean_absolute_error: 1.1759\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 5s 780ms/step - loss: 0.9313 - mean_absolute_error: 0.9313 - val_loss: 1.1718 - val_mean_absolute_error: 1.1718\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 5s 777ms/step - loss: 1.0866 - mean_absolute_error: 1.0866 - val_loss: 1.2924 - val_mean_absolute_error: 1.2924\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 6s 858ms/step - loss: 1.0290 - mean_absolute_error: 1.0290 - val_loss: 1.1685 - val_mean_absolute_error: 1.1685\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 5s 742ms/step - loss: 0.9342 - mean_absolute_error: 0.9342 - val_loss: 1.1780 - val_mean_absolute_error: 1.1780\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 5s 778ms/step - loss: 0.9248 - mean_absolute_error: 0.9248 - val_loss: 1.2164 - val_mean_absolute_error: 1.2164\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 6s 859ms/step - loss: 0.9781 - mean_absolute_error: 0.9781 - val_loss: 1.1857 - val_mean_absolute_error: 1.1857\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 5s 784ms/step - loss: 1.1246 - mean_absolute_error: 1.1246 - val_loss: 1.2348 - val_mean_absolute_error: 1.2348\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 5s 777ms/step - loss: 0.8536 - mean_absolute_error: 0.8536 - val_loss: 1.1633 - val_mean_absolute_error: 1.1633\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    datagen.flow(data_train, y_train, batch_size=64),\n",
    "    steps_per_epoch=453//64,\n",
    "    epochs=500,\n",
    "    validation_data=test_datagen.flow(data_test, y_test, batch_size=64),\n",
    "    callbacks=[earlystop, checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./models/love_classifierv4.1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choose candidates from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "[0.  0.  1.  3.  1.  0.  1.  0.  3.  4.  1.  1.  2.  0.  1.  1.  3.  1.\n",
      " 5.  4.  0.  3.  0.  5.  1.  2.  2.  4.  4.  1.  2.  1.  0.  2.  2.  1.\n",
      " 1.  1.  0.  1.  5.  0.  0.  0.  0.  1.  1.  1.  3.5 2.  1.  3.  0.  0.\n",
      " 4.  0.  0.  0.  2.  0.  1.  4.5 0.  2.  0.  3.  3.  5.  1.  1.  1.  2.\n",
      " 4.  4.5 0.  0.  4.  1.  0.  1.  5.  0.  2.  3.  1.  5.  2.  0.  1.  0.\n",
      " 0.  1.  5.  0.  5.  2.  5.  2.  3.  0.  4.  0.  0.  0.  4.  0.  1.  0.\n",
      " 0.  5.  2.  0.  0.  3.  1.  2.  1.  4.  1.  0.  0.  0.  3.  0.  2.  3.\n",
      " 0.  1.  5.  0.  2.  1.  3.  1.  0.  1.  3.  1.  1.  3.  0.  3.  1.  4.5\n",
      " 1.  0.  0.  0.  5.  0.  3.  2.  2.  5.  3.  1.  0.  4.  3.  0.  2.  0.\n",
      " 3.  1.  0.  2.  4.  1.  1.  0.  3.  0.  1.  5.  2.  0.  3.  1.  0.  1.\n",
      " 2.  1.  3.  2.  5.  1.  0.  4.  5.  3.  1.  4.  0.  4.5 5.  0.  5.  1.\n",
      " 4.  5.  2.  2.  2.  0.  2.  1.  0.  0.  3.  5.  0.  3.  1.  0.  2.  1.\n",
      " 5.  0.  2.  0.  4.  3.  0.  5.  1.  0.  1.  0.  0.  1.  0.  1.  2.  0.\n",
      " 3.  0.  2.  2.  0.  4.  5.  1.  0.  0.  3.  1.  2.  0.  2.  1.  0.  0.\n",
      " 0.  5.  3.  0.  1.  3.  0.  1.  0.  2.  2.  5.  2.  0.  0.  1.  0.  5.\n",
      " 1.  2.  1.  1.  0.  3.  5.  3.  3.  2.  0.  5.  0.  0.  2.  0.  4.  1.\n",
      " 4.5 1.  2.  2.  0.  2.  3.  3.  0.  4.  5.  3.  1.  0.  3.  0.  0.  3.\n",
      " 3.  0.  1.  0.  5.  4.  2.  0.  1.  4.  0.  1.  4.  0.  3.  2.  0.  1.\n",
      " 4.  5.  0.  5.  1.  1.  0.  1.  1.  1.  1.  3.  0.  0.  3.  2.  0.  0.\n",
      " 4.  1.  1.  1.  1.  0.  0.  1.  0.  5.  0.  0.  1.  5.  4.  0.  4.  0.\n",
      " 3.  0.  1.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  1.  1.  0.  3.  0.\n",
      " 0.  0.  3.  3.  3.  2.  5.  1.  0.  5.  1.  4.  1.  2.  1.  3.  2.  0.\n",
      " 2.  3.  0.  0.  2.  3.  0.  1.  1.  2.  3.  5.  5.  5.  4.  1.  2.  0.\n",
      " 0.  0.  0.  0.  2.  0.  3.  3.  2.  3.  3.  4.  1.  4.  2.  1.  1.  4.\n",
      " 1.  3.  2.  2.  0.  0.  0.  3.  0.  1.  1.  4.  0.  0.  3.  1.  1.  2.\n",
      " 0.  4.  4.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "#freeze layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "candidates = data_test\n",
    "candidates = np.expand_dims(candidates, axis=0)\n",
    "predictions = np.zeros(candidates.shape[1])\n",
    "\n",
    "for i in range(candidates.shape[1]):\n",
    "    predictions[i] = model.predict(candidates[:,i])\n",
    "\n",
    "#sort predictions\n",
    "predictions = predictions[np.argsort(predictions)]\n",
    "print(predictions)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^ Network should make more differentiated predictions. Like 3.36633825.\n",
    "See v4.2 for continuation of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
