{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input, Dropout\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_vggface import utils\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 200\n",
    "IMG_DIR = '../project/all_females'\n",
    "RATING_PATH = './Projekt_SGE_Assessment_ErikK.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable weights:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200, 200, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 200, 200, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 200, 200, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 100, 100, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 100, 100, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 50, 50, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 50, 50, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 50, 50, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 25, 25, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 25, 25, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 25, 25, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 14,879,041\n",
      "Trainable params: 164,353\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "vgg_base = VGGFace(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='max')\n",
    "\n",
    "# Add custom layers\n",
    "last_layer = vgg_base.get_layer('global_max_pooling2d_1').output\n",
    "#X = Dropout(0.2)(last_layer)\n",
    "# x = Flatten(name='flatten')(last_layer)\n",
    "X = Dense(256, activation='relu', name='fc6')(last_layer)\n",
    "X = Dropout(0.2)(X)\n",
    "X = Dense(128, activation='relu', name='fc7')(X)\n",
    "X = Dropout(0.2)(X)\n",
    "output = Dense(1, activation='linear')(X)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=vgg_base.input, outputs=output)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in vgg_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Make sure weights are not trainable\n",
    "print(\"Trainable weights:\")\n",
    "model.trainable_weights\n",
    "\n",
    "model.compile(optimizer=Adam(clipnorm=1.0),loss='mean_absolute_error', metrics=['mae'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "### **Please remember to delete the train & test folders if you have run this cell before!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test-split:\n",
      "X_train:  (454,)\n",
      "X_test:  (114,)\n",
      "y_train:  (454, 1)\n",
      "y_test:  (114, 1)\n"
     ]
    }
   ],
   "source": [
    "# Read in ratings\n",
    "ratings = np.genfromtxt(RATING_PATH)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "### Distribute images across folders\n",
    "if not os.path.isdir('./data/train/train'):\n",
    "    os.makedirs('./data/train/train')\n",
    "if not os.path.isdir('./data/test/test'):\n",
    "    os.makedirs('./data/test/test')\n",
    "        \n",
    "### Move all images according to ratings\n",
    "images = sorted(os.listdir(IMG_DIR))\n",
    "\n",
    "### Make sure lengths of ratings and images correspond \n",
    "assert len(ratings) == len(images)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, ratings, test_size=0.2)\n",
    "\n",
    "y_train = scaler.fit_transform(y_train.reshape(-1,1))\n",
    "y_test = scaler.transform(y_test.reshape(-1,1))\n",
    "    \n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "        \n",
    "print(\"Train-test-split:\")\n",
    "print(\"X_train: \",X_train.shape)\n",
    "print(\"X_test: \",X_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "data_train = np.zeros((X_train.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "data_test = np.zeros((X_test.shape[0], IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "for idx, _im in enumerate(X_train):\n",
    "    # Change the image path with yours.\n",
    "    _img = image.load_img(os.path.join(IMG_DIR, _im), target_size=(IMG_SIZE,IMG_SIZE))\n",
    "    _x = image.img_to_array(_img)\n",
    "    _x = np.expand_dims(_x, axis=0)\n",
    "    data_train[idx, :, :, :] = utils.preprocess_input(_x, version=1) / .255 \n",
    "    \n",
    "# Same for test data\n",
    "for idx, _im in enumerate(X_test):   \n",
    "    _img = image.load_img(os.path.join(IMG_DIR, _im), target_size=(IMG_SIZE,IMG_SIZE))\n",
    "    _x = image.img_to_array(_img)\n",
    "    _x = np.expand_dims(_x, axis=0)\n",
    "    data_test[idx, :, :, :] = utils.preprocess_input(_x, version=1) / .255\n",
    "\n",
    "### Loop throgh all images and store them in the dedicated folders\n",
    "#for img in X_train:\n",
    "#    src = os.path.join(IMG_DIR, img)\n",
    "#    dest = os.path.join('./data/train/train', img)\n",
    "#    shutil.copy(src, dest)\n",
    "\n",
    "#for img in X_test:\n",
    "#    src = os.path.join(IMG_DIR, img)\n",
    "#    dest = os.path.join('./data/test/test', img)\n",
    "#    shutil.copy(src, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, \n",
    "                             rotation_range=40,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode='nearest',\n",
    "                             preprocessing_function=utils.preprocess_input)\n",
    "\n",
    "datagen.fit(data_train)\n",
    "\n",
    "#train_generator = datagen.flow_from_directory(\n",
    "#    './data/train',\n",
    "#    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "#    batch_size=64,\n",
    "#    class_mode=None\n",
    "#)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen.fit(data_test)\n",
    "\n",
    "#test_generator = test_datagen.flow_from_directory(\n",
    "#    './data/test',\n",
    "#    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "#    batch_size=64,\n",
    "#    class_mode=None\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 70s 696ms/step - loss: 0.7944 - mean_absolute_error: 0.7944 - val_loss: 0.6610 - val_mean_absolute_error: 0.6610\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66096, saving model to ./weights-v4.4.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./weights-v4.4.hdf5', verbose=1, save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(data_train, y_train, batch_size=64),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=1,\n",
    "    validation_data=test_datagen.flow(data_test, y_test, batch_size=64),\n",
    "    callbacks=[checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "7/7 [==============================] - 6s 800ms/step - loss: 0.6900 - mean_absolute_error: 0.6900 - val_loss: 0.6841 - val_mean_absolute_error: 0.6841\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 5s 734ms/step - loss: 0.7168 - mean_absolute_error: 0.7168 - val_loss: 0.7365 - val_mean_absolute_error: 0.7365\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 5s 741ms/step - loss: 0.6790 - mean_absolute_error: 0.6790 - val_loss: 0.7184 - val_mean_absolute_error: 0.7184\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 6s 815ms/step - loss: 0.6571 - mean_absolute_error: 0.6571 - val_loss: 0.7089 - val_mean_absolute_error: 0.7089\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 5s 663ms/step - loss: 0.6486 - mean_absolute_error: 0.6486 - val_loss: 0.7219 - val_mean_absolute_error: 0.7219\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 6s 820ms/step - loss: 0.7059 - mean_absolute_error: 0.7059 - val_loss: 0.7322 - val_mean_absolute_error: 0.7322\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 5s 658ms/step - loss: 0.6502 - mean_absolute_error: 0.6502 - val_loss: 0.7621 - val_mean_absolute_error: 0.7621\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 6s 838ms/step - loss: 0.6509 - mean_absolute_error: 0.6509 - val_loss: 0.7217 - val_mean_absolute_error: 0.7217\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 5s 746ms/step - loss: 0.6851 - mean_absolute_error: 0.6851 - val_loss: 0.6916 - val_mean_absolute_error: 0.6916\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 5s 753ms/step - loss: 0.7044 - mean_absolute_error: 0.7044 - val_loss: 0.7688 - val_mean_absolute_error: 0.7688\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 5s 751ms/step - loss: 0.6840 - mean_absolute_error: 0.6840 - val_loss: 0.7141 - val_mean_absolute_error: 0.7141\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    datagen.flow(data_train, y_train, batch_size=64),\n",
    "    steps_per_epoch=453//64,\n",
    "    epochs=500,\n",
    "    validation_data=test_datagen.flow(data_test, y_test, batch_size=64),\n",
    "    callbacks=[earlystop, checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./models/love_classifierv4.4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choose candidates from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-203.15292358 -196.89613342 -177.4152832  -167.01104736 -161.61357117\n",
      " -150.23927307 -148.96322632 -133.67350769 -126.41686249 -126.15959167\n",
      " -123.8169632  -122.62071991 -122.06064606 -115.33239746 -115.03014374\n",
      " -114.93141174 -113.79706573 -112.21574402 -111.04689026 -108.41350555\n",
      " -103.25289917 -102.35009766  -92.88715363  -92.01108551  -90.55640411\n",
      "  -90.16567993  -85.33348083  -80.0843277   -77.27255249  -76.95850372\n",
      "  -76.64919281  -70.00288391  -68.67638397  -67.0359726   -66.89862061\n",
      "  -65.95204926  -63.45209503  -63.11006165  -63.04042435  -61.3217392\n",
      "  -61.08117676  -60.34743881  -54.58935928  -51.41580582  -49.91259766\n",
      "  -47.93086243  -45.05858994  -44.03202057  -32.59744644  -28.14681435\n",
      "  -27.83138657  -27.60424423  -27.12369728  -15.64478874  -14.73303509\n",
      "   -3.37487602    1.0468601     2.05631661    2.54559994    7.03844547\n",
      "   10.28412628   12.08301258   18.11764717   23.8874855    27.34967232\n",
      "   37.44688034   54.39094925   60.35200882   68.33992767   70.9205246\n",
      "   72.47158813   76.87793732   79.90798187   82.66860199   84.10173798\n",
      "   84.63530731   88.04065704  107.26073456  109.74312592  109.90066528\n",
      "  114.23303223  114.25745392  114.49504852  119.77108002  119.97214508\n",
      "  123.94012451  124.14925385  127.3736496   134.33927917  137.72492981\n",
      "  139.01850891  143.89251709  153.24092102  155.57194519  165.33337402\n",
      "  167.2350769   187.91038513  188.29756165  197.39422607  201.50862122\n",
      "  209.72296143  215.00375366  225.23768616  227.57809448  265.05154419\n",
      "  283.56286621  284.54431152  294.28503418  339.38952637  339.78121948\n",
      "  346.00784302  346.50979614  402.39416504  477.79788208]\n"
     ]
    }
   ],
   "source": [
    "#freeze layers\n",
    "#for layer in model.layers:\n",
    "#    layer.trainable = False\n",
    "\n",
    "candidates = data_test\n",
    "candidates = np.expand_dims(candidates, axis=0)\n",
    "predictions = np.zeros(candidates.shape[1])\n",
    "\n",
    "for i in range(candidates.shape[1]):\n",
    "    predictions[i] = model.predict(candidates[:,i])\n",
    "\n",
    "#sort predictions\n",
    "predictions = predictions[np.argsort(predictions)]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^ Output for regression should be between 0 & 5. Need to change activation of last layer, see v4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00658718]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 0.81530218]\n",
      " [ 2.02989509]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-0.39929073]\n",
      " [ 0.20800572]\n",
      " [ 0.81530218]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 1.42259863]\n",
      " [ 0.81530218]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [ 2.02989509]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 0.81530218]\n",
      " [ 0.81530218]\n",
      " [ 1.42259863]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [ 0.81530218]\n",
      " [ 0.81530218]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [ 1.72624686]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 2.02989509]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [ 0.81530218]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [ 1.42259863]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-0.39929073]\n",
      " [ 0.20800572]\n",
      " [ 1.42259863]\n",
      " [ 0.81530218]\n",
      " [-1.00658718]\n",
      " [ 1.42259863]\n",
      " [-1.00658718]\n",
      " [ 0.81530218]\n",
      " [ 1.42259863]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [-1.00658718]\n",
      " [ 2.02989509]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 0.81530218]\n",
      " [-0.39929073]\n",
      " [ 0.20800572]\n",
      " [ 0.81530218]\n",
      " [ 1.42259863]\n",
      " [ 0.20800572]\n",
      " [ 1.72624686]\n",
      " [ 0.20800572]\n",
      " [ 2.02989509]\n",
      " [ 2.02989509]\n",
      " [ 1.42259863]\n",
      " [ 1.42259863]\n",
      " [-1.00658718]\n",
      " [-0.39929073]\n",
      " [ 2.02989509]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 1.42259863]\n",
      " [ 0.20800572]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 2.02989509]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [-1.00658718]\n",
      " [ 1.42259863]\n",
      " [ 0.20800572]\n",
      " [-1.00658718]\n",
      " [-0.39929073]\n",
      " [-1.00658718]\n",
      " [ 0.20800572]\n",
      " [-1.00658718]\n",
      " [-0.39929073]\n",
      " [-1.00658718]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
