{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook is used for hyperparameter search for the rater CNN.\\nIt utilizes the Hyperopt library, as well as Hyperas, a wrapper for it to make defining \\nthe search space more readable.\\n\\nHyperopt: https://github.com/hyperopt/hyperopt\\nHyperas:  https://github.com/maxpumperla/hyperas\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This notebook is used for hyperparameter search for the rater CNN.\n",
    "It utilizes the Hyperopt library, as well as Hyperas, a wrapper for it to make defining \n",
    "the search space more readable.\n",
    "\n",
    "Hyperopt: https://github.com/hyperopt/hyperopt\n",
    "Hyperas:  https://github.com/maxpumperla/hyperas\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras import backend as K\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cfg = K.tf.ConfigProto()\n",
    "#cfg.gpu_options.allow_growth = True\n",
    "#K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Loads and prepares data.\n",
    "    Done in a separate function to load it only once, instead of at every evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #define paths\n",
    "    BASE_DIR = '../project/all_females' #image folder\n",
    "    RATING_DIR = './Projekt_SGE_Assessment_ErikK.txt' #text file with user ratings\n",
    "    \n",
    "    N_IMAGES = len(os.listdir(BASE_DIR))\n",
    "    IMAGE_SIZE = (200, 200)\n",
    "\n",
    "    X_train = np.zeros((N_IMAGES, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "    for idx, _im in enumerate(sorted(os.listdir(BASE_DIR))):\n",
    "        # Change the image path with yours.\n",
    "        _img = image.load_img(os.path.join(BASE_DIR, _im), target_size=IMAGE_SIZE)\n",
    "        _x = image.img_to_array(_img)\n",
    "        _x = np.expand_dims(_x, axis=0)\n",
    "        X_train[idx, :, :, :] = utils.preprocess_input(_x, version=1) / .255 \n",
    "    \n",
    "    assert ~np.any(np.isnan(X_train))\n",
    "    \n",
    "    # Load ratings\n",
    "    ratings = np.genfromtxt(RATING_DIR)\n",
    "    ratings = to_categorical(ratings, num_classes=6)\n",
    "    \n",
    "    # Standardize ratings\n",
    "    ratings_z = (ratings - np.mean(ratings)) / np.std(ratings)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_train, ratings_z, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    '''\n",
    "    Model to be optimized.\n",
    "    {{}} signify parameters which will be optimized.\n",
    "    \n",
    "    Returns:\n",
    "        loss - Evaluation metric to be optimized.\n",
    "        status - hyperopt.STATUS_OK because model always yields valid (albeit possibly bad) results\n",
    "        model - created model\n",
    "    '''\n",
    "    #K.clear_session()\n",
    "    #del history\n",
    "    #del model\n",
    "    #gc.collect()\n",
    "    #gc.collect()\n",
    "    #gc.collect()\n",
    "    #time.sleep(1)\n",
    "    \n",
    "    #limit_mem()\n",
    "    \n",
    "    #K.clear_session()\n",
    "    #tf.reset_default_graph()    \n",
    "\n",
    "    input_size = 200\n",
    "    vgg_base = VGGFace(include_top=False, input_shape=(input_size, input_size, 3), pooling='max')\n",
    "    \n",
    "    # Add custom layers\n",
    "    #last_layer = vgg_base.get_layer('global_max_pooling2d_1').output\n",
    "    last_layer = vgg_base.layers[-1].output\n",
    "    X = Dropout({{uniform(0, 1)}})(last_layer)\n",
    "    \n",
    "    X = Dense(256, kernel_regularizer=regularizers.l2({{uniform(0, 1)}}), activation={{choice(['relu', 'sigmoid'])}}, name='fc6')(X)\n",
    "    X = Dropout({{uniform(0, 1)}})(X)\n",
    "    \n",
    "    X = Dense(128, kernel_regularizer=regularizers.l2({{uniform(0, 1)}}), activation={{choice(['relu', 'sigmoid'])}}, name='fc7')(X)\n",
    "    X = Dropout({{uniform(0, 1)}})(X)\n",
    "    \n",
    "    if conditional({{choice(['two', 'three'])}}) == 'three':\n",
    "        #add another dense layer\n",
    "        X = Dense(128, kernel_regularizer=regularizers.l2({{uniform(0, 1)}}), activation={{choice(['relu', 'sigmoid'])}}, name='fc8')(X)\n",
    "        X = Dropout({{uniform(0, 1)}})(X)\n",
    "    \n",
    "    output = Dense(6, kernel_regularizer=regularizers.l2({{uniform(0, 1)}}), activation='softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=vgg_base.input, outputs=output)\n",
    "    \n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    for layer in vgg_base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    opt = Adam(lr={{uniform(0, 1)}},clipnorm=1.0)\n",
    "\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, ratings_z, batch_size=32, epochs=50, verbose=2, validation_data=(x_test, y_test))\n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    #save architecture as json for more memory efficiency\n",
    "    json_string = model.to_json()\n",
    "    \n",
    "    #del history\n",
    "    #del model\n",
    "    #gc.collect()\n",
    "    #gc.collect()\n",
    "    #gc.collect()\n",
    "    #time.sleep(1)    \n",
    "    \n",
    "    \n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': (json_string,opt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vgg_base = VGGFace(include_top=False, input_shape=(200, 200, 3), pooling='max')\n",
    "#vgg_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.engine import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Flatten, Dense, Input, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import image\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras_vggface.vggface import VGGFace\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras_vggface import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import regularizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import model_from_json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'Dropout_3': hp.uniform('Dropout_3', 0, 1),\n",
      "        'activation_1': hp.choice('activation_1', ['relu', 'sigmoid']),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'conditional': hp.choice('conditional', ['two', 'three']),\n",
      "        'Dropout_5': hp.uniform('Dropout_5', 0, 1),\n",
      "        'activation_2': hp.choice('activation_2', ['relu', 'sigmoid']),\n",
      "        'Dropout_6': hp.uniform('Dropout_6', 0, 1),\n",
      "        'Dropout_7': hp.uniform('Dropout_7', 0, 1),\n",
      "        'Dropout_8': hp.uniform('Dropout_8', 0, 1),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\"\n",
      "   3: Loads and prepares data.\n",
      "   4: Done in a separate function to load it only once, instead of at every evaluation.\n",
      "   5: \"\"\"\n",
      "   6: \n",
      "   7: #define paths\n",
      "   8: BASE_DIR = '../project/all_females' #image folder\n",
      "   9: RATING_DIR = './Projekt_SGE_Assessment_ErikK.txt' #text file with user ratings\n",
      "  10: \n",
      "  11: N_IMAGES = len(os.listdir(BASE_DIR))\n",
      "  12: IMAGE_SIZE = (200, 200)\n",
      "  13: \n",
      "  14: X_train = np.zeros((N_IMAGES, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
      "  15: \n",
      "  16: for idx, _im in enumerate(sorted(os.listdir(BASE_DIR))):\n",
      "  17:     # Change the image path with yours.\n",
      "  18:     _img = image.load_img(os.path.join(BASE_DIR, _im), target_size=IMAGE_SIZE)\n",
      "  19:     _x = image.img_to_array(_img)\n",
      "  20:     _x = np.expand_dims(_x, axis=0)\n",
      "  21:     X_train[idx, :, :, :] = utils.preprocess_input(_x, version=1) / .255 \n",
      "  22: \n",
      "  23: assert ~np.any(np.isnan(X_train))\n",
      "  24: \n",
      "  25: # Load ratings\n",
      "  26: ratings = np.genfromtxt(RATING_DIR)\n",
      "  27: ratings = to_categorical(ratings, num_classes=6)\n",
      "  28: \n",
      "  29: # Standardize ratings\n",
      "  30: ratings_z = (ratings - np.mean(ratings)) / np.std(ratings)\n",
      "  31: \n",
      "  32: x_train, x_test, y_train, y_test = train_test_split(X_train, ratings_z, test_size=0.2, random_state=42)\n",
      "  33: \n",
      "  34: \n",
      "  35: \n",
      "  36: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     '''\n",
      "   4:     Model to be optimized.\n",
      "   5:     {{}} signify parameters which will be optimized.\n",
      "   6:     \n",
      "   7:     Returns:\n",
      "   8:         loss - Evaluation metric to be optimized.\n",
      "   9:         status - hyperopt.STATUS_OK because model always yields valid (albeit possibly bad) results\n",
      "  10:         model - created model\n",
      "  11:     '''\n",
      "  12:     #K.clear_session()\n",
      "  13:     #del history\n",
      "  14:     #del model\n",
      "  15:     #gc.collect()\n",
      "  16:     #gc.collect()\n",
      "  17:     #gc.collect()\n",
      "  18:     #time.sleep(1)\n",
      "  19:     \n",
      "  20:     #limit_mem()\n",
      "  21:     \n",
      "  22:     #K.clear_session()\n",
      "  23:     #tf.reset_default_graph()    \n",
      "  24: \n",
      "  25:     input_size = 200\n",
      "  26:     vgg_base = VGGFace(include_top=False, input_shape=(input_size, input_size, 3), pooling='max')\n",
      "  27:     \n",
      "  28:     # Add custom layers\n",
      "  29:     #last_layer = vgg_base.get_layer('global_max_pooling2d_1').output\n",
      "  30:     last_layer = vgg_base.layers[-1].output\n",
      "  31:     X = Dropout(space['Dropout'])(last_layer)\n",
      "  32:     \n",
      "  33:     X = Dense(256, kernel_regularizer=regularizers.l2(space['Dropout_1']), activation=space['activation'], name='fc6')(X)\n",
      "  34:     X = Dropout(space['Dropout_2'])(X)\n",
      "  35:     \n",
      "  36:     X = Dense(128, kernel_regularizer=regularizers.l2(space['Dropout_3']), activation=space['activation_1'], name='fc7')(X)\n",
      "  37:     X = Dropout(space['Dropout_4'])(X)\n",
      "  38:     \n",
      "  39:     if conditional(space['conditional']) == 'three':\n",
      "  40:         #add another dense layer\n",
      "  41:         X = Dense(128, kernel_regularizer=regularizers.l2(space['Dropout_5']), activation=space['activation_2'], name='fc8')(X)\n",
      "  42:         X = Dropout(space['Dropout_6'])(X)\n",
      "  43:     \n",
      "  44:     output = Dense(6, kernel_regularizer=regularizers.l2(space['Dropout_7']), activation='softmax')(X)\n",
      "  45:     \n",
      "  46:     model = Model(inputs=vgg_base.input, outputs=output)\n",
      "  47:     \n",
      "  48:     # first: train only the top layers (which were randomly initialized)\n",
      "  49:     # i.e. freeze all convolutional InceptionV3 layers\n",
      "  50:     for layer in vgg_base.layers:\n",
      "  51:         layer.trainable = False\n",
      "  52: \n",
      "  53:     opt = Adam(lr=space['Dropout_8'],clipnorm=1.0)\n",
      "  54: \n",
      "  55:     model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
      "  56:     model.fit(X_train, ratings_z, batch_size=32, epochs=50, verbose=2, validation_data=(x_test, y_test))\n",
      "  57:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  58:     \n",
      "  59:     #save architecture as json for more memory efficiency\n",
      "  60:     json_string = model.to_json()\n",
      "  61:     \n",
      "  62:     #del history\n",
      "  63:     #del model\n",
      "  64:     #gc.collect()\n",
      "  65:     #gc.collect()\n",
      "  66:     #gc.collect()\n",
      "  67:     #time.sleep(1)    \n",
      "  68:     \n",
      "  69:     \n",
      "  70:     print('Test accuracy:', acc)\n",
      "  71:     return {'loss': -acc, 'status': STATUS_OK, 'model': (json_string,opt)}\n",
      "  72: \n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 1861.7524 - acc: 0.1426 - val_loss: 434.6018 - val_acc: 0.0877\n",
      "Epoch 2/3\n",
      " - 7s - loss: 332.1013 - acc: 0.1004 - val_loss: 225.8589 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 7s - loss: 186.2992 - acc: 0.2465 - val_loss: 72.8057 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 4561.7208 - acc: 0.1708 - val_loss: 960.2854 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 7s - loss: 1766.8565 - acc: 0.3046 - val_loss: 3979.0650 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 7s - loss: 1608.9406 - acc: 0.3275 - val_loss: 1162.9996 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 3239.3281 - acc: 0.2201 - val_loss: 572.9974 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 7s - loss: 346.2855 - acc: 0.1989 - val_loss: 163.0941 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 7s - loss: 197.5940 - acc: 0.1954 - val_loss: 433.3955 - val_acc: 0.0877\n",
      "Test accuracy: 0.08771929831097\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 1834.4290 - acc: 0.2606 - val_loss: 425.0611 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 7s - loss: 218.6419 - acc: 0.2852 - val_loss: 98.4362 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 7s - loss: 78.3762 - acc: 0.3046 - val_loss: 59.3886 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 1516.7567 - acc: 0.2923 - val_loss: 275.1772 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 7s - loss: 474.2921 - acc: 0.2606 - val_loss: 195.3861 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 7s - loss: 491.5299 - acc: 0.3099 - val_loss: 402.8741 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 3327.0060 - acc: 0.1954 - val_loss: 873.3517 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 7s - loss: 710.0371 - acc: 0.1162 - val_loss: 1921.7017 - val_acc: 0.1667\n",
      "Epoch 3/3\n",
      " - 8s - loss: 1622.6719 - acc: 0.1725 - val_loss: 903.2719 - val_acc: 0.0789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0789473684864086\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 8s - loss: 841.6241 - acc: 0.2905 - val_loss: 167.9681 - val_acc: 0.2719\n",
      "Epoch 2/3\n",
      " - 7s - loss: 86.8045 - acc: 0.3134 - val_loss: 28.0369 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 7s - loss: 23.5878 - acc: 0.3063 - val_loss: 12.2074 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 190.9213 - acc: 0.2993 - val_loss: 185.0769 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 172.9346 - acc: 0.3222 - val_loss: 259.7233 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 200.6317 - acc: 0.3292 - val_loss: 198.1981 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 10s - loss: 3861.1825 - acc: 0.3081 - val_loss: 1530.5921 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 728.3574 - acc: 0.3028 - val_loss: 814.4466 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 478.4507 - acc: 0.2905 - val_loss: 533.5249 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 155.8914 - acc: 0.3327 - val_loss: 76.4858 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 61.7688 - acc: 0.3292 - val_loss: 87.0218 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 80.8165 - acc: 0.2887 - val_loss: 64.4159 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 1806.6343 - acc: 0.3275 - val_loss: 339.1717 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 163.3156 - acc: 0.3415 - val_loss: 56.2938 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 8s - loss: 46.9996 - acc: 0.3363 - val_loss: 28.1081 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 10370.6726 - acc: 0.1268 - val_loss: 1594.6545 - val_acc: 0.0526\n",
      "Epoch 2/3\n",
      " - 8s - loss: 952.8813 - acc: 0.1708 - val_loss: 611.9957 - val_acc: 0.1667\n",
      "Epoch 3/3\n",
      " - 8s - loss: 838.7330 - acc: 0.1479 - val_loss: 323.1225 - val_acc: 0.1667\n",
      "Test accuracy: 0.1666666667973786\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 1455.3903 - acc: 0.2658 - val_loss: 349.5605 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 215.2717 - acc: 0.3134 - val_loss: 119.4675 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 85.3293 - acc: 0.3363 - val_loss: 67.0764 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 1740.2125 - acc: 0.2782 - val_loss: 476.8881 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 805.2929 - acc: 0.2799 - val_loss: 467.1737 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 8s - loss: 472.7126 - acc: 0.2658 - val_loss: 215.5375 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 12761.3124 - acc: 0.2606 - val_loss: 2210.7980 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 1144.2423 - acc: 0.3451 - val_loss: 457.7431 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 304.6022 - acc: 0.3134 - val_loss: 200.6251 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 10s - loss: 6302.0977 - acc: 0.2095 - val_loss: 1690.0584 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 1037.1426 - acc: 0.3204 - val_loss: 386.2784 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 298.1605 - acc: 0.3380 - val_loss: 354.0643 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 88.7052 - acc: 0.3187 - val_loss: 8.3435 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 0.6887 - acc: 0.3468 - val_loss: -4.7588e+00 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: -4.0952e+00 - acc: 0.3468 - val_loss: -6.5810e+00 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 3860.4945 - acc: 0.2975 - val_loss: 1240.2402 - val_acc: 0.0526\n",
      "Epoch 2/3\n",
      " - 8s - loss: 800.6909 - acc: 0.2465 - val_loss: 462.1016 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 7s - loss: 361.1716 - acc: 0.3380 - val_loss: 278.8919 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 351.2858 - acc: 0.1074 - val_loss: 176.0498 - val_acc: 0.0526\n",
      "Epoch 2/3\n",
      " - 7s - loss: 134.8396 - acc: 0.1215 - val_loss: 62.3889 - val_acc: 0.2368\n",
      "Epoch 3/3\n",
      " - 8s - loss: 111.2955 - acc: 0.1743 - val_loss: 224.2129 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 1581.7655 - acc: 0.2236 - val_loss: 256.8400 - val_acc: 0.2719\n",
      "Epoch 2/3\n",
      " - 8s - loss: 227.3317 - acc: 0.1655 - val_loss: 97.7877 - val_acc: 0.1667\n",
      "Epoch 3/3\n",
      " - 8s - loss: 108.1547 - acc: 0.1039 - val_loss: 71.1315 - val_acc: 0.0526\n",
      "Test accuracy: 0.05263157901272439\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 112.0259 - acc: 0.3187 - val_loss: 49.2353 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 48.0974 - acc: 0.3081 - val_loss: 41.9741 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 8s - loss: 39.8343 - acc: 0.3239 - val_loss: 47.9842 - val_acc: 0.2895\n",
      "Test accuracy: 0.2894736863019174\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 5788.1247 - acc: 0.3116 - val_loss: 1879.2752 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 1184.6084 - acc: 0.3134 - val_loss: 646.4928 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 661.3843 - acc: 0.3187 - val_loss: 802.6257 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 3342.9362 - acc: 0.2412 - val_loss: 704.4433 - val_acc: 0.0789\n",
      "Epoch 2/3\n",
      " - 8s - loss: 453.3880 - acc: 0.3046 - val_loss: 646.1863 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 457.2492 - acc: 0.2641 - val_loss: 220.2970 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 4330.3999 - acc: 0.2535 - val_loss: 997.6656 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 1355.2068 - acc: 0.3204 - val_loss: 433.4615 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 7s - loss: 520.4519 - acc: 0.2817 - val_loss: 279.3026 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 5841.8853 - acc: 0.3275 - val_loss: 1419.5496 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 7s - loss: 1845.7834 - acc: 0.3275 - val_loss: 948.6273 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 7s - loss: 995.4571 - acc: 0.3327 - val_loss: 1357.9082 - val_acc: 0.2807\n",
      "Test accuracy: 0.28070175517023654\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 102.2104 - acc: 0.3046 - val_loss: 38.9483 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 27.2969 - acc: 0.3222 - val_loss: 14.7303 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 12.1952 - acc: 0.3239 - val_loss: 13.0646 - val_acc: 0.2719\n",
      "Test accuracy: 0.27192982534567517\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 9s - loss: 821.5801 - acc: 0.2746 - val_loss: 282.1286 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 8s - loss: 210.9895 - acc: 0.3275 - val_loss: 164.7779 - val_acc: 0.2719\n",
      "Epoch 3/3\n",
      " - 8s - loss: 149.8621 - acc: 0.2958 - val_loss: 122.3337 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 10s - loss: 271.8674 - acc: 0.3468 - val_loss: 78.6229 - val_acc: 0.2719\n",
      "Epoch 2/3\n",
      " - 8s - loss: 46.1752 - acc: 0.3310 - val_loss: 22.7582 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 8s - loss: 24.2995 - acc: 0.2958 - val_loss: 26.3558 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 11s - loss: 66.2928 - acc: 0.2641 - val_loss: -5.0395e-02 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 9s - loss: -2.6337e+00 - acc: 0.3239 - val_loss: -6.9561e+00 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 9s - loss: -5.5446e+00 - acc: 0.3415 - val_loss: -6.7223e+00 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n",
      "Train on 568 samples, validate on 114 samples\n",
      "Epoch 1/3\n",
      " - 11s - loss: 970.3008 - acc: 0.2870 - val_loss: 349.1667 - val_acc: 0.3421\n",
      "Epoch 2/3\n",
      " - 9s - loss: 215.9310 - acc: 0.3239 - val_loss: 137.4789 - val_acc: 0.3421\n",
      "Epoch 3/3\n",
      " - 9s - loss: 119.4187 - acc: 0.3204 - val_loss: 150.1234 - val_acc: 0.3421\n",
      "Test accuracy: 0.3421052673406768\n"
     ]
    }
   ],
   "source": [
    "#run the hyperparameter search\n",
    "best_run, best_params = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=30,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='hyperSearch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "114/114 [==============================] - 1s 13ms/step\n",
      "205.21312164842035\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dropout': 0.8469072279547732, 'Dropout_1': 0.3210389307864787, 'Dropout_2': 0.5948712568443267, 'Dropout_3': 0.4970559482092457, 'Dropout_4': 0.06765709934504838, 'Dropout_5': 0.9662681038993752, 'Dropout_6': 0.011106434718081704, 'Dropout_7': 0.9770005173795487, 'Dropout_8': 0.8366666847115819, 'activation': 0, 'activation_1': 1, 'activation_2': 0, 'conditional': 0}\n"
     ]
    }
   ],
   "source": [
    "#recreate model from best_params\n",
    "K.clear_session()\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "#tf.reset_default_graph()\n",
    "best_archit, best_optim = best_params\n",
    "model = model_from_json(best_archit)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=best_optim)\n",
    "\n",
    "data_train, label_train, data_test, label_test = data()\n",
    "#print(data_test.shape)\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(model.evaluate(data_test, label_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save('opt_love_classifier_0.h5')\n",
    "\n",
    "#throws error: ValueError: Cannot use the \n",
    "#given session to evaluate tensor: the tensor's graph \n",
    "#is different from the session's graph.\n",
    "\n",
    "#couldn't resolve this, so model is saved as JSON + weight file instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "with open(\"opt_love_classifier_0.json\", \"w\") as json_file:\n",
    "    json_file.write(best_archit)\n",
    "model.save_weights(\"opt_love_classifier_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 11ms/step\n",
      "205.21312164842035\n"
     ]
    }
   ],
   "source": [
    "#reload model\n",
    "json_file = open('opt_love_classifier_0.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#load weights into new model\n",
    "loaded_model.load_weights(\"opt_love_classifier_0.h5\")\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=best_optim)\n",
    "print(model.evaluate(data_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
